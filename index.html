<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description"
          content="MoRF avatars are created from short monocular videos and can be rendered in real-time (30+ FPS, 640x640px) on mobile devices">
    <meta property="og:title" content="Map-Aware Human Pose Prediction for Robot Follow-Ahead"/>
    <meta property="og:description"
          content="Map-Aware Human Pose Prediction for Robot Follow-Ahead"/>
    <meta property="og:url" content="https://samsunglabs.github.io/c2g-HOF-project-page"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="Map-Aware Human Pose Prediction for Robot Follow-Ahead">
    <meta name="twitter:description"
          content="MoRF avatars are created from short monocular videos and can be rendered in real-time (30+ FPS, 640x640px) on mobile devices">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Map-Aware Human Pose Prediction for Robot Follow-Ahead</title>
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">
                        <a style="color:black;">Map-Aware Human Pose Prediction for Robot Follow-Ahead</a>
                    </h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                            <div class="author-portrait">
                              <img class="img-fluid mb-3" src="static/images/avatar/qingyuan.jpg" alt="">
                            </div>
                            <a href="https://scholar.google.com/citations?user=ob-N2PEAAAAJ&hl=en" target="_blank">Qingyuan<br>Jiang</a>
                        </span>

                        <span class="author-block">
                            <div class="author-portrait">
                              <img class="img-fluid mb-3" src="static/images/avatar/burak.jpg" alt="">
                            </div>
                            <a href="https://scholar.google.com.tr/citations?user=A-bqCfQAAAAJ&hl=tr" target="_blank">Burak<br>Susam</a>
                        </span>

                        <span class="author-block">
                            <div class="author-portrait">
                              <img class="img-fluid mb-3" src="static/images/avatar/junjee.jpeg" alt="">
                            </div>
                            <a href="https://scholar.google.com/citations?user=MXbgcw8AAAAJ&hl=en" target="_blank">Junjee<br>Chao</a>
                        </span>

                        <span class="author-block">
                            <div class="author-portrait">
                              <img class="img-fluid mb-3" src="static/images/avatar/volkan.jpg" alt="">
                            </div>
                            <a href="https://scholar.google.com/citations?user=Q5KT-hEAAAAJ&hl=en" target="_blank">Volkan<br>Isler</a>
                        </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block">Robotic Sensor Network Lab, University of Minnesota</span>
                    </div>

                    <!-- ArXiv Link -->
                    <span class="link-block">
                    <a href="" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Supplementary</span>
                    </a>
                    </span>

                    <span class="link-block">
                    <a href="https://drive.google.com/drive/folders/1jyxGQxHXxyNPKGvHbZcZ0bTUme2Bk3dV?usp=drive_link" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Real-IM Dataset</span>
                    </a>
                    </span>

                    <!-- Github link -->
                    <span class="link-block">
                    <a href="https://github.com/Qingyuan-Jiang/iros2024_poseForecasting" target="_blank"
                       class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                    </span>

                    <br>

                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="container">
                In the robot follow-ahead task, a mobile robot is tasked to maintain its relative position in front of a
                moving human actor while keeping the actor in sight.
                To accomplish this task it is important that the robot understand the full 3D pose of the human (since
                the head orientation can be different than the torso) and predict the future human poses so as to plan
                accordingly.
                The latter task is tricky in a complex environment with junctions and multiple corridors.
                In this work, we address the problem of forecasting the full 3D trajectory of a human in such
                environments.
                Our main insight is to show that one can first predict the 2D trajectory and then estimate the full 3D
                trajectory by conditioning the estimator on the predicted 2D trajectory.
                With this approach, we achieve results comparable or better than the state-of-the-art three times
                faster.
                As part of our contribution, we present a new dataset where, in contrast to existing datasets, the human
                motion is in a much larger area than a single room.
                We also present a complete robot system that integrates our human pose forecasting network on the mobile
                robot to enable real-time robot follow-ahead and present results from real-world experiments in multiple
                buildings on campus.
                <br>
                <br>
            </div>
            <div class="container" style="text-align:center;">
                <img src="static/images/teaser.png" alt="Main Idea" style="text-align:center;width:659px;"/>
            </div>
        </div>
    </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">

            <h2 class="title has-text-centered">Demonstration for real robot</h2>

            <div style="text-align: center;">
                <video poster="" id="demo1" autoplay controls muted loop playsinline width="75%" height="75%">
                    <source src="./static/videos/demo.mp4" type="video/mp4">
                </video>
            </div>
            <!-- End image carousel -->
        </div>
    </div>
</section>
<!-- End teaser video -->

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div style="text-align: center;">
                <h2 class="title has-text-centered">Planning results</h2>
                <img src="./static/videos/anim1.gif" alt="Path Planning " width="50%">
                <img src="./static/videos/anim2.gif" alt="Path Planning " width="50%" style="float:left">
            </div>
            <div style="text-align: center;">
                Planning based human motion prediction. Greedy v.s. Dynamic programming.
            </div>
        </div>
    </div>
</section>

<!--<div class="topnav">-->
<!--    <a href="#home">Home</a>-->
<!--    <a href="#news">Download</a>-->
<!--    <a href="#contact"><a href="https://github.com/Qingyuan-Jiang/iros2024_poseForecasting">Github</a>-->
<!--        <a href="#about"><a href="https://rsn.umn.edu/">About</a>-->
<!--</div>-->

<!--<div class="container">-->
<!--    <h1>Map-Aware Human Pose Prediction for Robot Follow-Ahead</h1>-->
<!--    <p><strong>Authors:</strong> Qingyuan Jiang, Burak Susam, Jun-Jee Chao, and Volkan Isler</p>-->
<!--    <p><strong>University of Minnesota RSN Laboratory</strong></p>-->

<!--    <h2>Introduction</h2>-->
<!--    <p>-->
<!--        We address the challenge of maintaining a robot's position ahead of-->
<!--        a moving actor by predicting the actor's 3D pose and movements in complex environments.-->
<!--        We propose a novel method that integrates environmental information for accurate,-->
<!--        long-term human pose prediction, overcoming limitations of existing methods in open spaces.-->
<!--        They introduce a new, realistic dataset for human motion in large areas and a robot system-->
<!--        equipped with dual cameras for localization and motion tracking. Our approach outperforms-->
<!--        existing models in trajectory and pose prediction, demonstrating real-time application in-->
<!--        robot follow-ahead tasks without relying on external cameras. Key contributions include a-->
<!--        real-time prediction method using occupancy maps and GRU, a new large-scale dataset,-->
<!--        and superior performance in both trajectory prediction and long-term human pose forecasting.-->
<!--    </p>-->
<!--    <figure>-->
<!--        <img src='static/images/teaser.png' class="responsive-image">-->
<!--        <figcaption>Figure 1: Map-aware Human Pose Prediction</figcaption>-->
<!--    </figure>-->

<!--    <h2>Approach</h2>-->
<!--    <p>Given human history poses and surrounding environment,-->
<!--        our method predicts the future poses by following a twostep pipeline: first predict a human-->
<!--        trajectory, then complete the full-body pose based on the-->
<!--        predicted trajectory. The following subsections introduce our-->
<!--        representation of the environment and human poses. Then,-->
<!--        we describe each component in our network architecture in-->
<!--        detail.</p>-->
<!--    <figure>-->
<!--        <img src='static/images/supplemantary/modules.png' class="responsive-image">-->
<!--        <figcaption>Figure 2: System Modules</figcaption>-->
<!--    </figure>-->
<!--    <figure>-->
<!--        <img src='static/images/network.png' class="responsive-image">-->
<!--        <figcaption>Figure 3: Network Architecture</figcaption>-->
<!--    </figure>-->

<!--    <h2>Dataset</h2>-->
<!--    <p>In addition to the standard synthetic dataset: GTA-IM,-->
<!--        we are also interested in evaluating our method on a largescale real-world dataset. However, existing realistic-->
<!--        datasets,-->
<!--        such as PROX [9], are limited to single-room areas. Therefore, we collect and propose the Real Indoor Motion-->
<!--        (RealIM) dataset for the large-scale human follow-ahead. We-->
<!--        simultaneously capture the entire human body and environment in building-scale spaces using the robot shown in-->
<!--        Figure 4. Compared to the existing synthetic dataset (GTAIM), the proposed dataset contains more movement-->
<!--        patterns,-->
<!--        including walking, crab moving, and varying moving speeds.-->
<!--        We collect 12 sequences from 5 different building halls.-->
<!--        Each contains an approximately four-minute movement. The-->
<!--        dataset includes sequences with different lighting conditions-->
<!--        recorded at different daytime. We invite multiple actors with-->
<!--        different walking styles and genders. We provide the raw-->
<!--        ROS bags and pre-processed data for direct use. In Fig. ??,-->
<!--        we present a few sample images from our dataset visualized-->
<!--        in Rviz</p>-->
<!--    <figure>-->
<!--        <img src='static/images/hardware.png' class="responsive-image">-->
<!--        <figcaption>Figure 4: Hardware Setup</figcaption>-->
<!--    </figure>-->
<!--    <figure>-->
<!--        <img src='static/images/dataset.png' class="responsive-image">-->
<!--        <figcaption>Figure 5: The Dataset and Collection Stages</figcaption>-->
<!--    </figure>-->


<!--    <h2>Results</h2>-->

<!--    <figure>-->
<!--        <img src='static/images/qualitative.png' class="responsive-image">-->
<!--        <figcaption>Figure 6: Qualitative results - Placeholder</figcaption>-->
<!--    </figure>-->

<!--    <section>-->
<!--        <h2>Setup</h2>-->
<!--        <p>After cloning the <a href="https://github.com/Qingyuan-Jiang/iros2024_poseForecasting">repo</a>, download the-->
<!--            data and place it in the data/ folder.</p>-->
<!--    </section>-->

<!--    <section>-->
<!--        <h2>Preprocess</h2>-->
<!--        <p>Execute <code>python process_real_dataset.py</code> to preprocess the data. This creates a <code>pf_preprocessed</code>-->
<!--            folder inside the data folder.</p>-->
<!--        <pre>-->
<!--        humanPosePrediction-->
<!--        ├── data-->
<!--        │   ├── Real-IM-Dataset-->
<!--        │   │   ├── sqeuence-1-->
<!--        │   │   │   ├── img1.jpg-->
<!--        │   ├── pf_preprocessed-->
<!--        │   │   ├── sequence-1.npz-->
<!--        ├── datasets-->
<!--        ├── results-->
<!--        </pre>-->
<!--    </section>-->

<!--    <section>-->
<!--        <h2>Training</h2>-->
<!--        <p>Initiate training by running <code>python train.py</code>. To train with more networks, add scripts in the-->
<!--            models folder and update the <code>models/motion_pred.py</code> file accordingly.</p>-->
<!--    </section>-->

<!--    <section>-->
<!--        <h2>Evaluation</h2>-->
<!--        <p>For evaluation, run <code>python eval.py</code> to visualize the results.</p>-->
<!--    </section>-->

<!--    <h2>References</h2>-->
<!--    <ul>-->
<!--        <li>A list of references used throughout the paper.</li>-->
<!--    </ul>-->
<!--</div>-->
</body>
</html>