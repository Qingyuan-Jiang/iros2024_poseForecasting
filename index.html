<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Map-Aware Human Pose Prediction for Robot Follow-Ahead</title>
<style>
    .table-container {
      text-align: center;
    }
    table {
      margin: auto;
    }
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    .container { max-width: 800px; margin: auto; }
    img { max-width: 100%; height: auto; }
    .image-placeholder { background-color: #f3f3f3; color: #333; text-align: center; padding: 20px; margin: 20px 0; }
    h1{ color: #333;
        text-align: center;}
    h2 { color: #333; }
    p { margin: 10px 0; }
    ul { margin: 10px 20px; }
    figcaption {
    text-align: center; /* Centers the text of figcaption */
    display: block; /* Ensures figcaption is treated as a block-level element */
    margin: auto; /* Centers the figcaption block if it has a specified width */
    }
    .responsive-image {
        max-width: 100%;
        height: auto;
    }
    tr {
        border-bottom: 1px solid #ddd;
    }
    td, th {
        text-align: center; /* Centers text horizontally */
        vertical-align: middle; /* Centers text vertically */
        border: 1px solid #ddd; /* Adds border to table data and header cells */
        padding: 8px; /* Optional: Adds some padding inside the cells */
    }
    tr:hover {background-color: #D6EEEE;}
    table {
        border-collapse: collapse;
        width: 100%;
    }
    .topnav {
    overflow: hidden;
    background-color: #333;
  }

  .topnav a {
    float: left;
    display: block;
    color: #f2f2f2;
    text-align: center;
    padding: 14px 16px;
    text-decoration: none;
  }

  .topnav a:hover {
    background-color: #ddd;
    color: black;
  }
</style>
</head>
<body>

<div class="topnav">
  <a href="#home">Home</a>
  <a href="#news">Download</a>
  <a href="#contact"><a href="https://github.com/Qingyuan-Jiang/iros2024_poseForecasting">Github</a>
  <a href="#about"><a href="https://rsn.umn.edu/">About</a>
</div>

<div class="container">
    <h1>Map-Aware Human Pose Prediction for Robot Follow-Ahead</h1>
    <p><strong>Authors:</strong> Qingyuan Jiang, Burak Susam, Jun-Jee Chao, and Volkan Isler</p>
    <p><strong>University of Minnesota RSN Laboratory</strong></p>

    <h2>Introduction</h2>
    <p>
        We address the challenge of maintaining a robot's position ahead of
        a moving actor by predicting the actor's 3D pose and movements in complex environments.
        We propose a novel method that integrates environmental information for accurate,
        long-term human pose prediction, overcoming limitations of existing methods in open spaces.
        They introduce a new, realistic dataset for human motion in large areas and a robot system
        equipped with dual cameras for localization and motion tracking. Our approach outperforms
        existing models in trajectory and pose prediction, demonstrating real-time application in
        robot follow-ahead tasks without relying on external cameras. Key contributions include a
        real-time prediction method using occupancy maps and GRU, a new large-scale dataset,
        and superior performance in both trajectory prediction and long-term human pose forecasting.
    </p>
    <figure>
        <img src='figures/teaser.png' class="responsive-image">
        <figcaption>Figure 1: Map-aware Human Pose Prediction</figcaption>
    </figure>

    <h2>Approach</h2>
    <p>Given human history poses and surrounding environment,
        our method predicts the future poses by following a twostep pipeline: first predict a human
        trajectory, then complete the full-body pose based on the
        predicted trajectory. The following subsections introduce our
        representation of the environment and human poses. Then,
        we describe each component in our network architecture in
        detail.</p>
    <figure>
        <img src='figures/supplemantary/modules.png' class="responsive-image">
        <figcaption>Figure 2: System Modules</figcaption>
    </figure>
    <figure>
        <img src='figures/network.png' class="responsive-image">
        <figcaption>Figure 3: Network Architecture</figcaption>
    </figure>

    <h2>Dataset</h2>
    <p>In addition to the standard synthetic dataset: GTA-IM,
        we are also interested in evaluating our method on a largescale real-world dataset. However, existing realistic datasets,
        such as PROX [9], are limited to single-room areas. Therefore, we collect and propose the Real Indoor Motion (RealIM) dataset for the large-scale human follow-ahead. We
        simultaneously capture the entire human body and environment in building-scale spaces using the robot shown in
        Figure 4. Compared to the existing synthetic dataset (GTAIM), the proposed dataset contains more movement patterns,
        including walking, crab moving, and varying moving speeds.
        We collect 12 sequences from 5 different building halls.
        Each contains an approximately four-minute movement. The
        dataset includes sequences with different lighting conditions
        recorded at different daytime. We invite multiple actors with
        different walking styles and genders. We provide the raw
        ROS bags and pre-processed data for direct use. In Fig. ??,
        we present a few sample images from our dataset visualized
        in Rviz</p>
        <figure>
            <img src='figures/hardware.png' class="responsive-image">
            <figcaption>Figure 4: Hardware Setup</figcaption>
        </figure>
        <figure>
            <img src='figures/dataset.png' class="responsive-image">
            <figcaption>Figure 5: The Dataset and Collection Stages</figcaption>
        </figure>


    <h2>Results</h2>
    <p>Below, you can see our quantitative and qualitative results on both GTA-IM and Real-IM datasets.</p>
    <div class='table-container'>
    <table>
        <thead>
          <tr>
            <th>&nbsp;</th>
            <th colspan="4"><strong>Path error (mm)</strong></th>
            <th colspan="5"><strong>3D Pose error (mm)</strong></th>
          </tr>
          <tr>
            <th><strong>Time (sec)</strong></th>
            <th>0.5</th>
            <th>1.0</th>
            <th>1.5</th>
            <th>2.0</th>
            <th>0.5</th>
            <th>1.0</th>
            <th>1.5</th>
            <th>2.0</th>
            <th>mean(2s)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>TR</td>
            <td>113.7</td>
            <td>187.4</td>
            <td>375.8</td>
            <td>471.2</td>
            <td>112.4</td>
            <td>116.4</td>
            <td>129.3</td>
            <td>139.8</td>
            <td>118.3</td>
          </tr>
          <tr>
            <td>LT</td>
            <td>104</td>
            <td>163</td>
            <td>219</td>
            <td>297</td>
            <td>91</td>
            <td>158</td>
            <td>237</td>
            <td>328</td>
            <td>173</td>
          </tr>
          <tr>
            <td>CA</td>
            <td>58.0</td>
            <td>103.2</td>
            <td>154.9</td>
            <td>221.7</td>
            <td><strong>50.8</strong></td>
            <td>67.5</td>
            <td>75.5</td>
            <td>86.9</td>
            <td><strong>61.4</strong></td>
          </tr>
          <tr>
            <td>Ours (Pathnet+GRU)</td>
            <td><strong>52.5</strong></td>
            <td><strong>99.6</strong></td>
            <td><strong>107.3</strong></td>
            <td><strong>113.8</strong></td>
            <td>51.1</td>
            <td><strong>63.6</strong></td>
            <td><strong>70.7</strong></td>
            <td><strong>75.0</strong></td>
            <td>62.9</td>
          </tr>
          <tr>
            <td>Pathnet(partial)</td>
            <td>123.8</td>
            <td>149.3</td>
            <td>237.0</td>
            <td>290.9</td>
            <td>72.9</td>
            <td>83.5</td>
            <td>114.7</td>
            <td>126.9</td>
            <td>110.4</td>
          </tr>
          <tr>
            <td>Pathnet(unknown)</td>
            <td>120.5</td>
            <td>145.1</td>
            <td>232.6</td>
            <td>286.1</td>
            <td>72.4</td>
            <td>82.8</td>
            <td>105.5</td>
            <td>118.0</td>
            <td>83.5</td>
          </tr>
          <tr>
            <td>Ours w/o <span>L_col</span></td>
            <td>112.8</td>
            <td>153.6</td>
            <td>243.4</td>
            <td>292.0</td>
            <td>71.6</td>
            <td>81.0</td>
            <td>100.6</td>
            <td>110.6</td>
            <td>81.0</td>
          </tr>
          <tr>
            <td>Ours w/o <span>L_map</span></td>
            <td>132.0</td>
            <td>184.4</td>
            <td>320.1</td>
            <td>365.1</td>
            <td>73.9</td>
            <td>86.3</td>
            <td>129.6</td>
            <td>146.1</td>
            <td>93.3</td>
          </tr>
        </tbody>
        <caption><strong>Evaluation results in GTA-IM dataset</strong>. Results show that our methods outperform the state-of-the-art methods. The results also show that by providing additional map information, trajectories are predicted significantly better, which yields lower pose errors.</caption>
      </table>
      </div>
      <div class="table-container">
      <table>
        <caption><strong>Evaluation results in Real-IM dataset</strong>. We report results across both 2s and 3s prediction horizons. Our method performs better than the baselines on both path and pose prediction. As expected, the error increases along with the horizon for all methods.</caption>
        <thead>
          <tr>
            <th></th>
            <th colspan="6"><strong>Path error (mm)</strong></th>
            <th colspan="6"><strong>3D Pose error (mm)</strong></th>
          </tr>
          <tr>
            <th><strong>Time (sec)</strong></th>
            <th>1.0</th>
            <th>1.5</th>
            <th>2.0</th>
            <th>3.0</th>
            <th>mean(2s)</th>
            <th>mean(3s)</th>
            <th>1.0</th>
            <th>1.5</th>
            <th>2.0</th>
            <th>3.0</th>
            <th>mean(2s)</th>
            <th>mean(3s)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>TR</td>
            <td>150.5</td>
            <td>249.1</td>
            <td>295.7</td>
            <td>439.1</td>
            <td>154.3</td>
            <td>240.3</td>
            <td>89.9</td>
            <td>91.8</td>
            <td>93.8</td>
            <td>101.2</td>
            <td>89.8</td>
            <td>92.4</td>
          </tr>
          <tr>
            <td>CA</td>
            <td>153.9</td>
            <td>255.7</td>
            <td>257.6</td>
            <td>374.3</td>
            <td>253.2</td>
            <td>257.4</td>
            <td><strong>67.9</strong></td>
            <td>80.3</td>
            <td>86.4</td>
            <td>109.2</td>
            <td>69.7</td>
            <td>80.7</td>
          </tr>
          <tr>
            <td>Ours (Pathnet+TR)</td>
            <td rowspan="2"><strong>145.8</strong></td>
            <td rowspan="2"><strong>165.7</strong></td>
            <td rowspan="2"><strong>186.9</strong></td>
            <td rowspan="2"><strong>240.2</strong></td>
            <td rowspan="2"><strong>150.9</strong></td>
            <td rowspan="2"><strong>178.4</strong></td>
            <td>96.8</td>
            <td>97.0</td>
            <td>98.1</td>
            <td>98.4</td>
            <td>96.5</td>
            <td>97.1</td>
          </tr>
          <tr>
            <td>Ours (Pathnet+GRU)</td>
            <td>69.9</td>
            <td><strong>75.5</strong></td>
            <td><strong>78.1</strong></td>
            <td><strong>84.5</strong></td>
            <td>70.7</td>
            <td><strong>75.0</strong></td>
          </tr>
          <tr>
            <td>Pathnet(partial)</td>
            <td>202.0</td>
            <td>319.5</td>
            <td>372.4</td>
            <td>511.5</td>
            <td>251.9</td>
            <td>350.7</td>
            <td>80.9</td>
            <td>88.6</td>
            <td>92.7</td>
            <td>100.8</td>
            <td>83.2</td>
            <td>92.9</td>
          </tr>
          <tr>
            <td>Pathnet(unknown)</td>
            <td>183.3</td>
            <td>316.7</td>
            <td>390.9</td>
            <td>583.2</td>
            <td>192.7</td>
            <td>308.8</td>
            <td>69.3</td>
            <td>76.6</td>
            <td>81.2</td>
            <td>101.9</td>
            <td><strong>69.3</strong></td>
            <td>76.5</td>
          </tr>
          <tr>
            <td>Ours w/o L_col</td>
            <td>204.4</td>
            <td>449.6</td>
            <td>341.0</td>
            <td>493.2</td>
            <td>418.0</td>
            <td>440.6</td>
            <td>101.5</td>
            <td>102.3</td>
            <td>95.2</td>
            <td>115.0</td>
            <td>99.8</td>
            <td>107.0</td>
          </tr>
          <tr>
            <td>Ours w/o L_map</td>
            <td>162.1</td>
            <td>271.8</td>
            <td>441.4</td>
            <td>763.3</td>
            <td>171.9</td>
            <td>316.1</td>
            <td>68.5</td>
            <td>80.4</td>
            <td>92.9</td>
            <td>132.0</td>
            <td>69.8</td>
            <td>90.5</td>
          </tr>
        </tbody>
      </table>
    </div>

    <figure>
        <img src='figures/qualitative.png' class="responsive-image">
        <figcaption>Figure 6: Qualitative results - Placeholder</figcaption>
    </figure>

    <section>
        <h2>Setup</h2>
        <p>After cloning the <a href="https://github.com/Qingyuan-Jiang/iros2024_poseForecasting">repo</a>, download the data and place it in the data/ folder.</p>
    </section>

    <section>
        <h2>Preprocess</h2>
        <p>Execute <code>python process_real_dataset.py</code> to preprocess the data. This creates a <code>pf_preprocessed</code> folder inside the data folder.</p>
        <pre>
        humanPosePrediction
        ├── data
        │   ├── Real-IM-Dataset
        │   │   ├── sqeuence-1
        │   │   │   ├── img1.jpg
        │   ├── pf_preprocessed
        │   │   ├── sequence-1.npz
        ├── datasets
        ├── results
        </pre>
    </section>

    <section>
        <h2>Training</h2>
        <p>Initiate training by running <code>python train.py</code>. To train with more networks, add scripts in the models folder and update the <code>models/motion_pred.py</code> file accordingly.</p>
    </section>

    <section>
        <h2>Evaluation</h2>
        <p>For evaluation, run <code>python eval.py</code> to visualize the results.</p>
    </section>

    <h2>References</h2>
    <ul>
    <li>A list of references used throughout the paper.</li>
    </ul>
</div>
</body>
</html>